import asyncio
import json
import os
from pathlib import Path
import subprocess
import time
from zapv2 import ZAPv2
from dotenv import load_dotenv
import websockets

load_dotenv()

class Crawler:
    def __init__(self, url, setting, hostname, unique_id):
        self.url = url
        self.setting = setting
        self.hostname = hostname 
        self.crawler_file_path = self.get_folder_path("Results")/ unique_id /"crawled_urls.txt"
    def start(self):
        output = subprocess.run(
            f"zap.sh -daemon -config api.key={os.getenv('ZAP_API_KEY')} -port {os.getenv('ZAP_PORT')} -host 0.0.0.0",
            shell=True,
            capture_output=True
        )
    def get_zap_container_ip(self):
        try:
            result = subprocess.run(
                ["docker", "inspect", "-f", "{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}", "zap_container"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=True
            )
            ip_address = result.stdout.strip()
            print(f"[INFO] Detected ZAP container IP: {ip_address}")
            return ip_address
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to get ZAP container IP: {e.stderr}")
            raise RuntimeError("Could not retrieve ZAP container IP")

    def run_spider_scan(self, zap):
        print("Starting basic Spider scan...")
        scanID = zap.spider.scan(self.url, recurse=True, subtreeonly=True)
        while int(zap.spider.status(scanID)) < 100:
            print(f'Spider progress: {zap.spider.status(scanID)}%')
            time.sleep(2)
        print("Spider scan completed!")
        self.save_crawled_urls(zap.core.urls())

    def run_ajax_spider_scan(self, zap):
        print("Starting AJAX Spider scan...")
        zap.ajaxSpider.scan(self.url)
        timeout = time.time() + 60 
        while True:
            status = zap.ajaxSpider.status
            if status != 'running':
                print(f"AJAX Spider scan completed with status: {status}")
                break
            if time.time() > timeout:
                print("Timeout reached, stopping AJAX Spider scan.")
                break
            time.sleep(2)
        self.save_crawled_urls(zap.core.urls())

    def save_crawled_urls(self, urls):
        with open(self.crawler_file_path, 'a') as file:
            for url in urls:
                file.write(url + '\n')
        print(f"Crawled URLs saved to {self.crawler_file_path}")
    
    def clear_zap_urls(self, zap):
        try:
            zap.core.new_session()
            print("Previous session data cleared.")
        except Exception as e:
            print(f"Error clearing session data: {e}")

    async def send_message(self, message):
        async with websockets.connect("ws://localhost:8001/ws") as websocket:
            await websocket.send(message)

    def get_folder_path(self, folder):
        current_path = None
        for root, dirs, files in os.walk(os.path.abspath('.')):
            if folder in dirs:
                current_path = os.path.join(root, folder)
                break
        if not current_path:
            print(f"Le dossier {folder} n'a pas été trouvé dans le projet.")
        return Path(current_path) 

    def get_file(self, *args):
        folder_path = self.get_folder_path("Results") 
        file_path = os.path.join(folder_path, *args)
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        if not os.path.exists(file_path):
            with open(file_path, 'w') as file:
                file.write("") 
        return file_path
    
    def crawler_txt_file(self, file_path):
        try:
            with open(file_path, 'r') as file:
                urls = file.readlines()

            unique_urls = list(set(url.strip() for url in urls))
            with open(file_path, 'w') as file:
                for url in unique_urls:
                    file.write(url + '\n')
            print(f"Duplicates removed. Unique URLs saved to {file_path}")
        
        except Exception as e:
            print(f"Error: {e}")

    def spider(self, zap):
        print("the scan begin, check for setting")
        self.clear_zap_urls(zap)
        if self.setting['zap_d'] > 0 and self.setting['zap_dc'] == 0:
            print("spider option1")
            zap.spider.set_option_max_depth(self.setting['zap_d'])
            self.run_spider_scan(zap)
        elif self.setting['zap_dc'] > 0 and self.setting['zap_d'] == 0:
            print("spider option2")
            zap.spider.set_option_max_children(self.setting['zap_dc'])
            self.run_spider_scan(zap)
        elif self.setting['zap_dc'] > 0 and self.setting['zap_d'] > 0:
            print("spider option3")
            zap.spider.set_option_max_depth(self.setting['zap_d'])
            zap.spider.set_option_max_children(self.setting['zap_dc'])
            self.run_spider_scan(zap)
        elif self.setting['zap_d'] == 0 and self.setting['zap_dc'] == 0:
            print("spider option4")
            print("the active scan begin")
            self.run_spider_scan(zap)
        time.sleep(1)
        self.run_ajax_spider_scan(zap)
        self.crawler_txt_file(self.crawler_file_path)

    def start_spider_scan(self):
        print("Starting combined Spider and AJAX Spider scan...")
        zap_container_ip = self.get_zap_container_ip() 
        zap = ZAPv2(apikey=os.getenv('ZAP_API_KEY'), 
           proxies={ 
                'http': f'http://{zap_container_ip}:{os.getenv("ZAP_PORT")}', 
                'https': f'http://{zap_container_ip}:{os.getenv("ZAP_PORT")}' 
        })
        self.spider(zap)  

    def start_authenticated_spider_scan(self, auth_credentials):
        print("Starting authenticated spider scan...")
        zap_container_ip = self.get_zap_container_ip() 
        zap = ZAPv2(apikey=os.getenv('ZAP_API_KEY'), 
           proxies={ 
                'http': f'http://{zap_container_ip}:{os.getenv("ZAP_PORT")}', 
                'https': f'http://{zap_container_ip}:{os.getenv("ZAP_PORT")}' 
        })
        zap.authentication.set_authentication_credentials(auth_credentials)
        self.spider(zap)  
 
       